{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Engine Implementation in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import streamlit as st\n",
    "from llama_index import GPTSimpleVectorIndex, SimpleDocument\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation Section\n",
    "# Overview\n",
    "\"\"\"\n",
    "The Content Engine is designed to analyze and compare multiple PDF documents, extracting insights\n",
    "and highlighting differences using Retrieval Augmented Generation (RAG). It incorporates local embedding models\n",
    "and a Large Language Model (LLM) to ensure data privacy. This notebook showcases the implementation\n",
    "of a Content Engine using LangChain, Streamlit, and ChromaDB.\n",
    "\"\"\"\n",
    "\n",
    "# Installation Instructions\n",
    "\"\"\"\n",
    "1. Install required dependencies:\n",
    "   ```bash\n",
    "   pip install streamlit langchain llama-index chromadb pypdf2\n",
    "   ```\n",
    "2. Ensure the local embedding model and LLM are properly configured.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Parse PDF Documents\n",
    "def parse_documents(file_paths):\n",
    "    documents = []\n",
    "    for file_path in file_paths:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents.extend(loader.load())\n",
    "    return documents\n",
    "\n",
    "# Initialize Embedding Model and Vector Store\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "vectorstore = None  # Placeholder for Chroma vector store\n",
    "\n",
    "# Function to Generate and Store Embeddings\n",
    "def generate_store_embeddings(documents):\n",
    "    global vectorstore\n",
    "    vectorstore = Chroma.from_documents(documents, embedding_model)\n",
    "\n",
    "# Setup Query Engine\n",
    "def setup_query_engine():\n",
    "    llm = OpenAI()  # Replace with a local LLM for privacy\n",
    "    return RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streamlit Integration for User Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.title(\"Content Engine for Document Analysis\")\n",
    "\n",
    "uploaded_files = st.file_uploader(\n",
    "    \"Upload PDF documents for analysis\", type=[\"pdf\"], accept_multiple_files=True\n",
    ")\n",
    "\n",
    "if uploaded_files:\n",
    "    file_paths = [uploaded_file.name for uploaded_file in uploaded_files]\n",
    "    documents = parse_documents(file_paths)\n",
    "    st.info(\"Documents successfully loaded and parsed.\")\n",
    "\n",
    "    generate_store_embeddings(documents)\n",
    "    st.success(\"Embeddings generated and stored locally.\")\n",
    "\n",
    "    qa_chain = setup_query_engine()\n",
    "    st.success(\"Query Engine configured successfully!\")\n",
    "\n",
    "    st.subheader(\"Ask your queries!\")\n",
    "    query = st.text_input(\"Enter your question:\")\n",
    "    if query:\n",
    "        response = qa_chain.run(query)\n",
    "        st.write(f\"Answer: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "# 1. Replace `OpenAI()` with a local LLM instance to ensure local processing.\n",
    "# 2. Test the Streamlit app by running the script from the terminal:\n",
    "#    ```bash\n",
    "#    streamlit run app.py\n",
    "#    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
